import os
import logging
import asyncio
import aiohttp
import time
from typing import Optional, Dict, Any
from io import BytesIO
import re


# Document processing libraries
try:
    import PyPDF2
    import docx
    from bs4 import BeautifulSoup
    import requests
except ImportError as e:
    logging.warning(f"Some document processing libraries not available: {e}")

from app.riskassessment.services.bedrock_service import BedrockService
from app.riskassessment.helpers.improved_pdf_extractor import ImprovedPDFExtractor
from app.riskassessment.helpers.dynamic_summary_config import DynamicSummaryConfig
from app.riskassessment.config import (
    BEDROCK_RT, 
    MODEL_MAPPING, 
    CONVERSATION_CHAT_MODEL_NAME,
    CONVERSATION_CHAT_TOP_P,
    CONVERSATION_CHAT_TEMPERATURE,
    LLM_MAX_TOKENS
)

# Initialize logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TextSummaryService:
    def __init__(self):
        """
        Initialize the Text Summary Service with AI models
        Uses same configuration as conversation service
        """
        self.openai_service = None
        self.bedrock_service = None
        
        # Get model configuration from environment
        model_name = CONVERSATION_CHAT_MODEL_NAME or "claude-37-sonnet"
        temperature = float(CONVERSATION_CHAT_TEMPERATURE or "0.6")
        top_p = float(CONVERSATION_CHAT_TOP_P or "0.6")
        max_tokens = int(LLM_MAX_TOKENS or "8192")
        
        logger.info(f"Initializing Text Summary Service with model: {model_name}")
        
        try:
            # Initialize Bedrock service with Claude 3.7 (Primary)
            if model_name in MODEL_MAPPING:
                bedrock_model_id = MODEL_MAPPING[model_name]
                self.bedrock_service = BedrockService(
                    model_id=bedrock_model_id,
                    temperature=temperature,
                    top_p=top_p,
                    max_tokens=max_tokens
                )
                logger.info(f"✅ Initialized Bedrock service with {model_name}: {bedrock_model_id}")
            else:
                logger.warning(f"Model {model_name} not found in MODEL_MAPPING")
        except Exception as e:
            logger.warning(f"Bedrock service not available: {e}")
        
        try:
            # Initialize OpenAI service (Fallback)
            self.openai_service = OpenAIService(
                model_id="gpt-3.5-turbo",
                temperature=temperature,
                top_p=top_p
            )
            logger.info("✅ Initialized OpenAI service as fallback")
        except Exception as e:
            logger.warning(f"OpenAI service not available: {e}")

    async def summarize_text(
        self, 
        text: str, 
        summary_type: str = "general", 
        max_length: int = 3000, 
        language: str = "vietnamese",
        auto_adjust_length: bool = True
    ) -> Dict[str, Any]:
        """
        Summarize text using AI models (simplified version)
        """
        try:
            # Validate input
            if not text or len(text.strip()) < 50:
                raise ValueError("Văn bản quá ngắn để tóm tắt (tối thiểu 50 ký tự)")
            
            # Clean and prepare text
            cleaned_text = self._clean_text(text)
            
            # Auto-adjust max_length based on document size
            if auto_adjust_length:
                optimal_max_length, analysis = DynamicSummaryConfig.calculate_optimal_max_length(
                    cleaned_text, summary_type, max_length
                )
                
                # Use optimal length if significantly different from user input
                if abs(optimal_max_length - max_length) > max_length * 0.5:
                    logger.info(f"Auto-adjusting max_length: {max_length} → {optimal_max_length}")
                    max_length = optimal_max_length
                
                # Add analysis to response
                document_analysis = analysis
            else:
                document_analysis = None
            
            # Generate prompt based on summary type and language
            prompt = self._generate_summary_prompt(
                text=cleaned_text,
                summary_type=summary_type,
                max_length=max_length,
                language=language
            )
            
            # Try AI services
            summary = None
            model_used = None
            start_time = time.time()
            
            # Try Bedrock first
            if self.bedrock_service:
                try:
                    logger.info("🤖 Using Bedrock service for summarization")
                    response = await self.bedrock_service.ai_ainvoke(prompt)
                    summary = self._extract_summary_from_response(response)
                    model_used = "bedrock_claude"
                    logger.info(f"✅ Bedrock summarization successful: {len(summary)} characters")
                except Exception as e:
                    logger.warning(f"Bedrock service failed: {str(e)}")
            
            # Fallback to OpenAI if Bedrock fails
            if not summary and self.openai_service:
                try:
                    logger.info("🤖 Falling back to OpenAI service")
                    response = await self.openai_service.ai_ainvoke(prompt)
                    summary = self._extract_summary_from_response(response)
                    model_used = "openai_gpt"
                    logger.info(f"✅ OpenAI summarization successful: {len(summary)} characters")
                except Exception as e:
                    logger.warning(f"OpenAI service failed: {str(e)}")
            
            if not summary:
                raise ValueError("Không có AI service nào khả dụng để tóm tắt")
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # Prepare response
            response = {
                "summary": summary,
                "summary_type": summary_type,
                "language": language,
                "original_length": len(text),
                "summary_length": len(summary),
                "compression_ratio": round(len(text) / len(summary), 2),
                "word_count": {
                    "original": len(text.split()),
                    "summary": len(summary.split())
                },
                "max_length_used": max_length,
                "processing_method": "direct_ai_call",
                "model_used": model_used,
                "processing_time": round(processing_time, 2)
            }
            
            # Add document analysis if available
            if document_analysis:
                response["document_analysis"] = document_analysis
            
            logger.info(f"✅ Text summarization complete: {processing_time:.2f}s using {model_used}")
            return response
            
        except Exception as e:
            logger.error(f"Error in text summarization: {str(e)}")
            raise
    
    async def _summarize_large_document(
        self,
        text: str,
        summary_type: str,
        max_length: int,
        language: str,
        document_analysis: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """Summarize large document using enhanced Bedrock service"""
        
        if not self.bedrock_service:
            raise ValueError("Bedrock service not available for large document processing")
        
        # Import enhanced service
        from app.riskassessment.services.enhanced_bedrock_service import EnhancedBedrockService
        
        # Create enhanced service
        enhanced_service = EnhancedBedrockService(self.bedrock_service)
        
        # Estimate processing requirements
        processing_estimate = enhanced_service.estimate_processing_time(text)
        logger.info(f"📊 Processing estimate: {processing_estimate}")
        
        # Process document
        start_time = time.time()
        
        result = await enhanced_service.summarize_large_document(
            text=text,
            summary_type=summary_type,
            max_length=max_length,
            language=language,
            max_chunks=10,  # Limit chunks for performance
            use_parallel_processing=True
        )
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        # Prepare response
        response = {
            "summary": result.summary,
            "summary_type": summary_type,
            "language": language,
            "original_length": len(text),
            "summary_length": len(result.summary),
            "compression_ratio": round(len(text) / len(result.summary), 2) if result.summary else 0,
            "word_count": {
                "original": len(text.split()),
                "summary": len(result.summary.split()) if result.summary else 0
            },
            "max_length_used": max_length,
            "processing_method": "enhanced_bedrock_chunking",
            "processing_stats": {
                **result.processing_stats,
                "actual_processing_time": round(processing_time, 2),
                "estimated_vs_actual": {
                    "estimated_seconds": processing_estimate["estimated_time_seconds"],
                    "actual_seconds": processing_time,
                    "accuracy": round(abs(processing_estimate["estimated_time_seconds"] - processing_time) / processing_estimate["estimated_time_seconds"] * 100, 1)
                }
            },
            "chunk_summaries": result.chunk_summaries,
            "source_info": result.source_info
        }
        
        # Add document analysis if available
        if document_analysis:
            response["document_analysis"] = document_analysis
        
        logger.info(f"✅ Large document summarization complete: {processing_time:.2f}s")
        return response
    
    async def _summarize_standard_document(
        self,
        text: str,
        summary_type: str,
        max_length: int,
        language: str,
        document_analysis: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """Summarize standard-sized document using direct Bedrock call"""
        
        # Generate prompt based on summary type and language
        prompt = self._generate_summary_prompt(
            text=text,
            summary_type=summary_type,
            max_length=max_length,
            language=language
        )
        
        # Try Bedrock first, then fallback to OpenAI
        summary = None
        model_used = None
        
        start_time = time.time()
        
        if self.bedrock_service:
            try:
                logger.info("🤖 Using Bedrock service for summarization")
                response = await self.bedrock_service.ai_ainvoke(prompt)
                summary = self._extract_summary_from_response(response)
                model_used = "bedrock_claude"
            except Exception as e:
                logger.warning(f"Bedrock service failed: {str(e)}")
        
        if not summary and self.openai_service:
            try:
                logger.info("🤖 Falling back to OpenAI service")
                response = await self.openai_service.ai_ainvoke(prompt)
                summary = self._extract_summary_from_response(response)
                model_used = "openai_gpt"
            except Exception as e:
                logger.warning(f"OpenAI service failed: {str(e)}")
        
        if not summary:
            raise ValueError("Không có AI service nào khả dụng để tóm tắt")
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        # Prepare response
        response = {
            "summary": summary,
            "summary_type": summary_type,
            "language": language,
            "original_length": len(text),
            "summary_length": len(summary),
            "compression_ratio": round(len(text) / len(summary), 2),
            "word_count": {
                "original": len(text.split()),
                "summary": len(summary.split())
            },
            "max_length_used": max_length,
            "processing_method": "direct_ai_call",
            "model_used": model_used,
            "processing_time": round(processing_time, 2)
        }
        
        # Add document analysis if available
        if document_analysis:
            response["document_analysis"] = document_analysis
        
        logger.info(f"✅ Standard document summarization complete: {processing_time:.2f}s using {model_used}")
        return response
        
        

    async def extract_text_from_document(
        self, 
        file_content: bytes, 
        file_extension: str, 
        filename: str,
        max_pages: Optional[int] = None
    ) -> str:
        """
        Extract text from various document formats
        
        Args:
            file_content: File content as bytes
            file_extension: File extension (.pdf, .docx, etc.)
            filename: Original filename
            max_pages: Maximum pages to process (None = all pages)
        """
        try:
            if file_extension == '.txt':
                return file_content.decode('utf-8')
            
            elif file_extension == '.pdf':
                return self._extract_text_from_pdf(file_content, max_pages)
            
            elif file_extension in ['.docx', '.doc']:
                return self._extract_text_from_docx(file_content)
            
            else:
                raise ValueError(f"Unsupported file format: {file_extension}")
                
        except Exception as e:
            logger.error(f"Error extracting text from {filename}: {str(e)}")
            raise

    def _extract_text_from_pdf(self, file_content: bytes, max_pages: Optional[int] = None) -> str:
        """
        Extract text from PDF file using ImprovedPDFExtractor
        
        Args:
            file_content: PDF file content as bytes
            max_pages: Maximum pages to process (None = all pages)
        """
        try:
            extractor = ImprovedPDFExtractor()
            # Pass max_pages to the extractor if it supports it
            extraction_result = extractor.extract_text_from_pdf(file_content, max_pages=max_pages)
            
            # Extract text and source information
            extracted_text = extraction_result['text']
            source = extraction_result['source']
            method = extraction_result['method']
            
            if not extracted_text.strip():
                raise ValueError("Không thể trích xuất text từ PDF")
            
            # Save extracted text to file based on source
            if source == 'pypdf2':
                self._save_extracted_text_to_file(extracted_text, "pdf_extracted_text.txt", method)
                logger.info(f"✅ PDF extraction successful via {method}: {len(extracted_text)} characters")
            elif source == 'ocr':
                # OCR already saves its own file, just log the success
                logger.info(f"✅ PDF extraction successful via {method}: {len(extracted_text)} characters")
                logger.info("📁 OCR result already saved to ocr_extracted_text.txt")
            
            return extracted_text
            
        except Exception as e:
            logger.error(f"Error extracting text from PDF: {str(e)}")
            raise

    def _extract_summary_from_response(self, response) -> str:
        """Extract summary text from AI service response"""
        try:
            if hasattr(response, 'content'):
                return response.content.strip()
            elif isinstance(response, str):
                return response.strip()
            elif isinstance(response, dict):
                return response.get('content', str(response)).strip()
            else:
                return str(response).strip()
        except Exception as e:
            logger.error(f"Error extracting summary from response: {e}")
            return "Lỗi trích xuất kết quả từ AI service"
        """
        Save extracted text to logs directory for debugging
        """
        try:
            import os
            from datetime import datetime
            
            # Create logs directory if it doesn't exist (Docker path)
            logs_dir = "/app/logs"
            os.makedirs(logs_dir, exist_ok=True)
            
            # Add timestamp to filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename_with_timestamp = f"{timestamp}_{filename}"
            file_path = os.path.join(logs_dir, filename_with_timestamp)
            
            # Save text to file
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write("="*80 + "\n")
                f.write(f"EXTRACTED TEXT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Extraction Method: {method}\n")
                f.write("="*80 + "\n\n")
                f.write(text)
                f.write("\n\n" + "="*80 + "\n")
                f.write(f"Total characters: {len(text)}\n")
                f.write(f"Total words: {len(text.split())}\n")
                f.write(f"Extraction method: {method}\n")
                f.write("="*80 + "\n")
            
            logger.info(f"📁 Extracted text saved to: {file_path}")
            
        except Exception as e:
            logger.warning(f"Failed to save extracted text to file: {e}")

    def _extract_text_from_docx(self, file_content: bytes) -> str:
        """
        Extract text from DOCX file
        """
        try:
            doc_file = BytesIO(file_content)
            doc = docx.Document(doc_file)
            
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            
            if not text.strip():
                raise ValueError("Không thể trích xuất text từ DOCX")
            
            return text
            
        except Exception as e:
            logger.error(f"Error extracting text from DOCX: {str(e)}")
            raise

    def _clean_text(self, text: str) -> str:
        """
        Clean and normalize text
        """
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove special characters but keep Vietnamese characters
        text = re.sub(r'[^\w\s\u00C0-\u1EF9.,!?;:()\-"]', '', text)
        
        return text.strip()

    def _generate_summary_prompt(
        self, 
        text: str, 
        summary_type: str, 
        max_length: int, 
        language: str
    ) -> str:
        """
        Generate clean, direct prompt for better AI response
        """
        if language == "vietnamese":
            if summary_type == "bullet_points":
                instruction = f"Tóm tắt văn bản sau thành {max_length} từ dưới dạng các điểm chính:"
            elif summary_type == "key_insights":
                instruction = f"Trích xuất những insight quan trọng nhất từ văn bản sau trong {max_length} từ:"
            elif summary_type == "executive_summary":
                instruction = f"Viết tóm tắt điều hành ngắn gọn từ văn bản sau trong {max_length} từ:"
            elif summary_type == "detailed":
                instruction = f"Tóm tắt chi tiết văn bản sau trong {max_length} từ:"
            else:  # general
                instruction = f"Tóm tắt văn bản sau trong {max_length} từ:"
        else:  # English
            if summary_type == "bullet_points":
                instruction = f"Summarize the following text in {max_length} words as bullet points:"
            elif summary_type == "key_insights":
                instruction = f"Extract key insights from the following text in {max_length} words:"
            elif summary_type == "executive_summary":
                instruction = f"Write an executive summary of the following text in {max_length} words:"
            elif summary_type == "detailed":
                instruction = f"Provide a detailed summary of the following text in {max_length} words:"
            else:  # general
                instruction = f"Summarize the following text in {max_length} words:"
        
        # Simple, direct prompt
        prompt = f"{instruction}\n\n{text}"
        return prompt

    async def _get_ai_summary(self, prompt: str) -> str:
        """
        Get summary from available AI services with response cleaning
        Priority: Bedrock (Claude 3.7) -> OpenAI -> Extractive
        """
        try:
            # Try Bedrock (Claude 3.7) first
            if self.bedrock_service:
                try:
                    logger.info("🤖 Using Bedrock Claude 3.7 for summarization")
                    response = await self.bedrock_service.ai_ainvoke(prompt)
                    summary = self._clean_ai_response(response.content.strip(), prompt)
                    logger.info(f"✅ Bedrock summary generated: {len(summary)} chars")
                    return summary
                except Exception as e:
                    logger.warning(f"Bedrock service failed: {e}")
            
            
            # Fallback to simple extractive summarization
            logger.info("🤖 Using extractive summarization as last resort")
            return self._simple_extractive_summary(prompt)
            
        except Exception as e:
            logger.error(f"All AI services failed: {e}")
            raise ValueError("Không thể tạo tóm tắt. Vui lòng thử lại sau.")

    def _clean_ai_response(self, response: str, original_prompt: str) -> str:
        """
        Clean AI response to extract only the summary content
        """
        try:
            # If response contains the original prompt, it's an echo - extract text only
            if len(response) > len(original_prompt) * 0.8:
                # Response is too similar to prompt, likely an echo
                logger.warning("Detected prompt echo, extracting text content")
                
                # Try to find the original text in the response
                lines = response.split('\n')
                text_lines = []
                skip_next = False
                
                for line in lines:
                    line = line.strip()
                    # Skip instruction lines
                    if any(keyword in line.lower() for keyword in ['tóm tắt', 'summarize', 'văn bản', 'yêu cầu', 'text']):
                        if len(line) < 100:  # Short instruction lines
                            continue
                    
                    # Keep content lines
                    if len(line) > 20 and not line.endswith(':'):
                        text_lines.append(line)
                
                if text_lines:
                    # Use the longest meaningful line as summary
                    response = max(text_lines, key=len)
                else:
                    # Fallback: use extractive summary
                    return self._simple_extractive_summary(response, max_sentences=2)
            
            # Clean up the response
            response = response.strip()
            
            # Remove common instruction echoes
            patterns_to_remove = [
                r'^.*?tóm tắt.*?từ[:\.]?\s*',
                r'^.*?summarize.*?words[:\.]?\s*',
                r'^.*?văn bản sau.*?[:\.]?\s*',
                r'yêu cầu:.*?$',
                r'tóm tắt:\s*$'
            ]
            
            for pattern in patterns_to_remove:
                response = re.sub(pattern, '', response, flags=re.IGNORECASE | re.MULTILINE)
            
            # Clean whitespace
            response = re.sub(r'\s+', ' ', response).strip()
            
            # If response is too short, use extractive fallback
            if len(response) < 30:
                logger.warning("Response too short after cleaning, using extractive fallback")
                return self._simple_extractive_summary(original_prompt, max_sentences=2)
            
            return response
            
        except Exception as e:
            logger.error(f"Error cleaning AI response: {e}")
            # Return extractive summary as safe fallback
            return self._simple_extractive_summary(original_prompt, max_sentences=2)

    def _simple_extractive_summary(self, input_text: str, max_sentences: int = 3) -> str:
        """
        Simple extractive summarization as fallback
        """
        try:
            # If input_text is a prompt, extract the actual text content
            text = input_text
            if '\n\n' in input_text:
                # Split by double newlines and take the longest part (likely the content)
                parts = input_text.split('\n\n')
                text = max(parts, key=len)
            
            # Clean the text
            text = re.sub(r'^\s*tóm tắt.*?:', '', text, flags=re.IGNORECASE)
            text = re.sub(r'^\s*summarize.*?:', '', text, flags=re.IGNORECASE)
            text = text.strip()
            
            # Extract sentences
            sentences = re.split(r'[.!?]+', text)
            sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
            
            if not sentences:
                return "Không thể tạo tóm tắt từ văn bản này."
            
            if len(sentences) <= max_sentences:
                return '. '.join(sentences) + '.'
            
            # Simple scoring based on sentence length and position
            scored_sentences = []
            for i, sentence in enumerate(sentences):
                score = len(sentence.split())  # Word count
                if i < len(sentences) * 0.3:  # Early sentences get bonus
                    score *= 1.2
                scored_sentences.append((score, sentence))
            
            # Sort by score and take top sentences
            scored_sentences.sort(reverse=True)
            top_sentences = [sent for _, sent in scored_sentences[:max_sentences]]
            
            return '. '.join(top_sentences) + '.'
            
        except Exception as e:
            logger.error(f"Simple summarization failed: {e}")
            return "Không thể tạo tóm tắt từ văn bản này."


# Legacy class for backward compatibility
class TextSummary:
    def __init__(self, model_path=None):
        self.service = TextSummaryService()
        logging.warning("TextSummary class is deprecated. Use TextSummaryService instead.")
